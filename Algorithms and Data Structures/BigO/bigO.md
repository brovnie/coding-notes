#BigO

**Big O Notation** is a way of describing the relationship between the input to a function or as it grows and how that changes the runtime of that function, the relatinship between the input size and the time relative to that input.

We say that an algorithm is O(f(n))) if the number of simple operations the computer has to do is eventually less than a constant times f(n), as n increases

f(n) could be:

- linear (f(n) = n)
- quadratic (f(n) = n(power2))
- constant (f(n) = 1 )
- something entirely different
